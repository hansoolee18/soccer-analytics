{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import optuna\n",
        "from skopt.searchcv import BayesSearchCV\n",
        "from sklearn_nature_inspired_algorithms.model_selection import NatureInspiredSearchCV\n",
        "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC"
      ],
      "metadata": {
        "id": "UVNcwa8ZoOTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import shap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.trial import Trial\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, LeaveOneOut, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, make_scorer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.base import clone\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "F_s2NEw1qyom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "fpeUy9_foQpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fifa=pd.read_excel('fifa_final.xlsx')\n"
      ],
      "metadata": {
        "id": "BB33M_d9oVpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(fifa.head())\n",
        "print(fifa.shape)"
      ],
      "metadata": {
        "id": "-ITsLA2woa8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_new=fifa.drop(['Name','Country','Position','Club','ID','Foot','BP','Joined','League','Loan Date End','A/W','D/W'], axis=1)"
      ],
      "metadata": {
        "id": "u5vh_EM1obDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns',200)"
      ],
      "metadata": {
        "id": "2pN1SehTobJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fifa_new"
      ],
      "metadata": {
        "id": "3YsccGv4oiiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target=fifa_new.Value\n",
        "fifa_indi=fifa_new.drop(['Value'],axis=1)"
      ],
      "metadata": {
        "id": "pHrH9ds0okzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain, xtest, ytrain, ytest=train_test_split(fifa_indi, target, test_size=0.2)"
      ],
      "metadata": {
        "id": "4K4bDM5Iolbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nrmsle(y, y_pred):\n",
        "    return -1 *np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "def rmsle(y, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "\n",
        "class optm():\n",
        "    def __init__(self):\n",
        "        None\n",
        "    \n",
        "    def run_optm(self, x_train, y_train, n_trial=10, direction='maximize', model = 'gbr'):\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        model_dict = {'gbr' : self.gbr_objective, \n",
        "                     'lgbr' : self.lgbr_objective,\n",
        "                      'xgbr' : self.xgbr_objective,\n",
        "                      'catbr' : self.catbr_objective\n",
        "                     }\n",
        "        optuna.logging.set_verbosity(0)\n",
        "        self.study = optuna.create_study(direction=direction)\n",
        "        self.study.optimize(model_dict[model], n_trials=n_trial)\n",
        "        \n",
        "        bestparams = self.study.best_params\n",
        "        best_score = self.study.best_value\n",
        "        print(f\"Best score:{best_score} \\nOptimized parameters: {bestparams}\")\n",
        "\n",
        "        return bestparams, best_score\n",
        "            \n",
        "    def plot_optm(self):\n",
        "\n",
        "        optuna.visualization.plot_optimization_history(self.study)\n",
        "        optuna.visualization.plot_param_importances(self.study)\n",
        "\n",
        "    def gbr_objective(self, trial):\n",
        "        gbm_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt']),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.1),\n",
        "        'random_state': 42\n",
        "        }\n",
        "\n",
        "        self.model = GradientBoostingRegressor(**gbm_params)\n",
        "        score = make_scorer(nrmsle, greater_is_better=True)\n",
        "        self.model = self.model.fit(self.x_train, self.y_train)\n",
        "        score = score(self.model, self.x_train, self.y_train)\n",
        "\n",
        "        return score\n",
        "      \n",
        "    def lgbr_objective(self, trial):\n",
        "        lgbm_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0, step=0.1),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.1),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 5.0), # lambda_l1\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 5.0), # lambda_l2\n",
        "        'random_state': 42\n",
        "        }\n",
        "\n",
        "        self.model = lgb.LGBMRegressor(**lgbm_params)\n",
        "        score = make_scorer(nrmsle, greater_is_better=True)\n",
        "        self.model = self.model.fit(self.x_train, self.y_train)\n",
        "        score = score(self.model, self.x_train, self.y_train)\n",
        "        \n",
        "        return score\n",
        "\n",
        "    def xgbr_objective(self, trial):\n",
        "        xgb_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 9),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.1),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.1),\n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 5.0, step=0.1),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 5.0), # lambda_l1\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 5.0), # lambda_l2\n",
        "        'random_state': 42\n",
        "        }\n",
        "\n",
        "        self.model = XGBRegressor(**xgb_params)\n",
        "        score = make_scorer(nrmsle, greater_is_better=True)\n",
        "        self.model = self.model.fit(self.x_train, self.y_train)\n",
        "        score = score(self.model, self.x_train, self.y_train)\n",
        "        \n",
        "        return score\n",
        "    \n",
        "    def catbr_objective(self, trial):\n",
        "        catboost_params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
        "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0, step=0.1),\n",
        "            'l2_leaf_reg' : trial.suggest_uniform('l2_leaf_reg', 1e-3, 10.0),\n",
        "            'bootstrap_type' : trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
        "            # 'max_bin' : trial.suggest_int('max_bin', 200, 400),\n",
        "            # 'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'boosting_type' : trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
        "            'random_state': 42\n",
        "            }\n",
        "\n",
        "        if catboost_params['bootstrap_type'] == 'Bayesian':\n",
        "            catboost_params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
        "        elif catboost_params['bootstrap_type'] == 'Bernoulli':\n",
        "            catboost_params['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n",
        "        elif catboost_params['bootstrap_type'] == 'MVS':\n",
        "            catboost_params['subsample'] = trial.suggest_uniform('subsample', 0.4, 1.0)\n",
        "\n",
        "        self.model = CatBoostRegressor(**catboost_params)\n",
        "        score = make_scorer(nrmsle, greater_is_better=True)\n",
        "        self.model = self.model.fit(self.x_train, self.y_train)\n",
        "        score = score(self.model, self.x_train, self.y_train)\n",
        "        \n",
        "        return score    \n"
      ],
      "metadata": {
        "id": "F28wAfwJoljT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_models = optm()\n",
        "opt_model = optm()\n",
        "\n",
        "def model_select(model_name, params):\n",
        "    if model_name == 'gbr':\n",
        "        b_model = GradientBoostingRegressor(**params)\n",
        "    elif model_name == 'lgbr':\n",
        "        b_model = lgb.LGBMRegressor(**params)\n",
        "    elif model_name == 'xgbr':\n",
        "        b_model = XGBRegressor(**params)\n",
        "    elif model_name == 'catbr':\n",
        "        b_model = CatBoostRegressor(**params)\n",
        "    return b_model\n",
        "        \n",
        "\n",
        "def mk_stacking_data(train_x, train_y, cv, model='gbr', trial=10):\n",
        "        train_fold_predict = np.zeros((train_x.shape[0], 1))\n",
        "        params_dict = {}\n",
        "        score_list = []\n",
        "        \n",
        "        for i, (trn_idx, val_idx) in enumerate(cv.split(train_x)) :\n",
        "            x_train, y_train = train_x[trn_idx, :], train_y[trn_idx]\n",
        "            x_val, y_val = train_x[val_idx, :], train_y[val_idx]\n",
        "            params, scores = opt_models.run_optm(x_train=x_train, y_train=y_train, n_trial=trial, direction='maximize', model=model)\n",
        "            params_dict[i] = params\n",
        "            score_list.append((scores, i))\n",
        "            \n",
        "            b_model = model_select(model_name=model, params=params)\n",
        "            \n",
        "            b_model.fit(x_train, y_train)\n",
        "            \n",
        "            train_fold_predict[val_idx, :] = b_model.predict(x_val).reshape(-1,1)\n",
        "\n",
        "        score_list.sort(key = lambda x:(x[0], -x[1]),reverse=True)\n",
        "\n",
        "        best_id = score_list[0][1]\n",
        "        b_model = model_select(model_name = model, params=params_dict[best_id])\n",
        "\n",
        "        return train_fold_predict, b_model\n",
        "\n",
        "    \n",
        "\n",
        "def run_inner_cv(train_x, train_y, cv, model='gbr', trial=10):\n",
        "\n",
        "        params_dict = {}\n",
        "        score_list = []\n",
        "        \n",
        "        for i, (trn_idx, val_idx) in enumerate(cv.split(train_x)) :\n",
        "            x_train, y_train = train_x[trn_idx, :], train_y[trn_idx]\n",
        "            x_val, y_val = train_x[val_idx, :], train_y[val_idx]\n",
        "            params, scores = opt_model.run_optm(x_train=x_train, y_train=y_train, n_trial=trial, direction='maximize', model=model)\n",
        "            params_dict[i] = params\n",
        "            score_list.append((scores, i))\n",
        "\n",
        "        score_list.sort(key = lambda x:(x[0], -x[1]),reverse=True)\n",
        "\n",
        "        best_id = score_list[0][1]\n",
        "        b_model = model_select(model_name = model, params=params_dict[best_id])\n",
        "\n",
        "        return b_model\n"
      ],
      "metadata": {
        "id": "aJbL6qtEoln1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### nested cross validataion and stacking ####\n",
        "\n",
        "model_list = ['gbr', 'lgbr', 'xgbr', 'catbr']\n",
        "\n",
        "X = fifa_indi.values\n",
        "y = target.values\n",
        "\n",
        "out_fold_n, inner_fold_n = 5, 5\n",
        "\n",
        "cv_outer = StratifiedKFold(n_splits=out_fold_n, shuffle=True, random_state=202)\n",
        "\n",
        "score_dict = {}\n",
        "model_dict = {}\n",
        "valid_train = []\n",
        "shap_values_list, test_sets = [], []\n",
        "\n",
        "for i,(train_ix, test_ix) in enumerate(cv_outer.split(X,y)):\n",
        "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "    Y_train, Y_test = y[train_ix], y[test_ix]\n",
        "    \n",
        "    new_train = np.array([])\n",
        "    new_test = np.array([])\n",
        "    cv_inner = KFold(n_splits=inner_fold_n, shuffle=True, random_state=1)\n",
        "    \n",
        "    valid_train = [(mk_stacking_data(train_x=X_train, train_y=Y_train, cv=cv_inner, model=m, trial=100)) for m in model_list]\n",
        "    \n",
        "    for name, (n_tr, b_m) in zip(model_list, valid_train):\n",
        "        new_train = np.hstack((new_train, n_tr.ravel()))\n",
        "        b_m.fit(X_train, Y_train)\n",
        "        test_result = b_m.predict(X_test)\n",
        "        score_dict[name] = nrmsle(Y_test, test_result)\n",
        "        model_dict[name] = b_m\n",
        "        new_test = np.hstack((new_test, test_result))\n",
        "\n",
        "        \n",
        "    xtrain_n = new_train.reshape(len(valid_train), -1).T\n",
        "    xtest_n = new_test.reshape(len(valid_train), -1).T\n",
        "    \n",
        "    reverse_dict = dict(map(reversed, score_dict.items()))\n",
        "    meta_model = reverse_dict[max(score_dict.values())]\n",
        "    \n",
        "    f_model = model_select(meta_model, params=model_dict[meta_model].get_params()) \n",
        "    f_model.fit(xtrain_n, Y_train)\n",
        "    y_hat = f_model.predict(xtest_n)\n",
        "    print(f'OUT_FOLD_{i}_Stacking_SCORE : ', rmsle(Y_test, y_hat))\n",
        "    \n",
        "    # shap_value \n",
        "    f_model = model_select(meta_model, params=model_dict[meta_model].get_params()) \n",
        "    f_model.fit(X_train, Y_train)\n",
        "    explainer = shap.TreeExplainer(f_model)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    shap_values_list.append(shap_values)\n",
        "    test_sets.append(test_ix)\n",
        "  "
      ],
      "metadata": {
        "id": "ffzJMrJho3mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_sets[0]\n",
        "shap_values = np.array(shap_values_list)[0]\n",
        "\n",
        "X_test = pd.DataFrame(X[test_set],columns=fifa_indi.columns)\n",
        "display(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "[shap.dependence_plot(i, shap_values, X_test) for i in range(X_test.shape[1])]"
      ],
      "metadata": {
        "id": "R5bshhsWTftf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oiPuF33rTNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### nested cross validataion ####\n",
        "\n",
        "model = 'gbr' # 'lgbr, xgbr, catbr'\n",
        "\n",
        "X = fifa_indi.values\n",
        "y = target.values\n",
        "\n",
        "out_fold_n, inner_fold_n = 5, 5\n",
        "\n",
        "cv_outer = StratifiedKFold(n_splits=out_fold_n, shuffle=True, random_state=202)\n",
        "\n",
        "shap_values_list, test_sets = [], []\n",
        "\n",
        "\n",
        "for i,(train_ix, test_ix) in enumerate(cv_outer.split(X,y)):\n",
        "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "    Y_train, Y_test = y[train_ix], y[test_ix]\n",
        "    \n",
        "    cv_inner = KFold(n_splits=inner_fold_n, shuffle=True, random_state=1)\n",
        "    b_model = run_inner_cv(train_x=X_train, train_y=Y_train, cv=cv_inner, model=model, trial=10)\n",
        "\n",
        "    f_model = model_select(model_name=model, params=b_model.get_params()) \n",
        "    f_model.fit(X_train, Y_train)\n",
        "    y_hat = f_model.predict(X_test)\n",
        "    print(f'OUT_FOLD_{i}_Stacking_SCORE : ', rmsle(Y_test, y_hat))\n",
        "    \n",
        "    # shap_value \n",
        "    explainer = shap.TreeExplainer(f_model)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    shap_values_list.append(shap_values)\n",
        "    test_sets.append(test_ix)\n",
        "    "
      ],
      "metadata": {
        "id": "2VyivvjYrTVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_sets[0]\n",
        "shap_values = np.array(shap_values_list)[0]\n",
        "\n",
        "X_test = pd.DataFrame(X[test_set],columns=fifa_indi.columns)\n",
        "display(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "[shap.dependence_plot(i, shap_values, X_test) for i in range(X_test.shape[1])]"
      ],
      "metadata": {
        "id": "5i-rZgtVrXUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}